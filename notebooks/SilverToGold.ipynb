{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a64a9541-b201-4f9e-9eb0-d8d88ca4bd2f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Incremental files ingestion from Azure Blob Storage to Microsoft Fabric Lakehouse using PySpark Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f080b-9110-4a4e-a126-d16476525595",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> <br> This project uses the database provided by the government of Brazil called **Cadastro Geral de Empregados e Desempregados (CAGED)** (General Register of Employed and Unemployed).<br><br>\n",
    "> This dataset consists of monthly .txt files and is made available on ftp for public download.  <br>  \n",
    "> Download available at: <br> \n",
    "> <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf5425-d92f-456e-9abf-143818a6f8c2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Notebook**: SilverToGold  \n",
    "**Description**: This PySpark notebook performs incremental ingestion of rows from the Silver layer of the Lakehouse to the Gold layer. It uses the ingestion date of each original file as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9ac1aa-0653-46aa-8a6c-99cc468b7813",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Set case sensitive for table and column names\n",
    "spark.conf.set('spark.sql.caseSensitive', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ebfc8c-8f44-4f09-bf91-d06f8889a0e7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a64cf-6430-4e11-b238-a0932766e91b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Lakehouse paths\n",
    "silver_meta_table         = \"CagedSilverMeta\"\n",
    "silver_table              = \"CagedSilver\"\n",
    "gold_meta_table         = \"CagedGoldMeta\"  # Control table to track from Silver (Curated) to Gold\n",
    "gold_table              = \"CagedGold\"      # Gold layer of data (Aggregated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc8560-b6c3-471d-971b-16326f1f491b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Prepare, create and load schemas and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655bf37-aa59-47af-8577-592edc48cdef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Define schema to gold_meta_table \n",
    "gold_meta_schema = StructType([\n",
    "    StructField(\"file_path\", StringType(), False),\n",
    "    StructField(\"source_modified_at\", TimestampType(), False),\n",
    "    StructField(\"processed_at\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "# Create the gold_meta_table (if not exists)\n",
    "spark.createDataFrame([], gold_meta_schema) \\\n",
    "    .write.format(\"delta\") \\\n",
    "    .mode(\"ignore\") \\\n",
    "    .saveAsTable(gold_meta_table)\n",
    "\n",
    "# Define schema to gold_table with types and PascalCase column names\n",
    "gold_schema = StructType([\n",
    "    StructField(\"CompetenciaMov\", DateType(), True),  \n",
    "    StructField(\"Regiao\", StringType(), True),\n",
    "    StructField(\"Uf\", StringType(), True),\n",
    "    StructField(\"Municipio\", StringType(), True),\n",
    "    StructField(\"Secao\", StringType(), True),\n",
    "    StructField(\"Subclasse\", StringType(), True),\n",
    "    StructField(\"SaldoMovimentacao\", IntegerType(), True),\n",
    "    StructField(\"GrauInstrucao\", IntegerType(), True),\n",
    "    StructField(\"Idade\", IntegerType(), True),\n",
    "    StructField(\"RacaCor\", StringType(), True),\n",
    "    StructField(\"Sexo\", StringType(), True),\n",
    "    StructField(\"TamEstabJan\", StringType(), True),\n",
    "    # Control Columns\n",
    "    StructField(\"FilePath\", StringType(), False),\n",
    "    StructField(\"SourceModifiedAt\", TimestampType(), False),\n",
    "    StructField(\"IsActive\", BooleanType(), False)\n",
    "])\n",
    "\n",
    "# Create the gold_table (if not exists)\n",
    "spark.createDataFrame([], gold_schema) \\\n",
    "    .write.format(\"delta\") \\\n",
    "    .mode(\"ignore\") \\\n",
    "    .partitionBy(\"CompetenciaMov\") \\\n",
    "    .saveAsTable(gold_table)\n",
    "\n",
    "# Load silver_table and gold_meta_table\n",
    "df_silver = spark.table(silver_table)\n",
    "df_gold_meta = spark.table(gold_meta_table)\n",
    "\n",
    "# Consolidate the files already processed on silver\n",
    "df_gold_meta_latest = df_gold_meta.groupBy(\"file_path\").agg(\n",
    "    F.max(\"source_modified_at\").alias(\"last_processed_source_mtime\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd46f51-49a2-4c12-960c-9e715ac492a3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b1bfd-4b60-49ea-b1a3-27258a517899",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Find candidates to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ee654-311e-4abe-ba2e-65aa2e299c82",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Identify candidate rows for ingestion (newers or updated)\n",
    "df_gold_candidates = (\n",
    "    df_silver\n",
    "    .join(df_gold_meta_latest, df_silver.file_path == df_gold_meta_latest.file_path, \"left\")\n",
    "    .filter(\n",
    "        F.col(\"last_processed_source_mtime\").isNull() |\n",
    "        (F.col(\"source_modified_at\") > F.coalesce(F.col(\"last_processed_source_mtime\"), F.lit(\"1970-01-01\")))\n",
    "    )\n",
    "    .select(\n",
    "        df_silver.file_path,\n",
    "        df_silver.source_modified_at,\n",
    "        df_silver.is_active,\n",
    "        # Select all columns except\n",
    "        *[F.col(col) for col in df_silver.columns if col not in [\"file_path\", \"source_modified_at\", \"is_active\"]]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Count candidates\n",
    "print(f\"Rows to process from Silver to Gold: {df_gold_candidates.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a669acb-f952-42d8-8eb3-b1e33457a337",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Process the load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95707bd2-3afa-4ebc-a4bb-a37339fcb292",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Process candidate rows\n",
    "\n",
    "if df_gold_candidates.rdd.isEmpty():\n",
    "    print(\"Nothing to load. Skipping...\")\n",
    "else:\n",
    "    # Transform the data\n",
    "    df_transformed = df_gold_candidates.select(\n",
    "        F.to_date(F.col(\"competênciamov\"), \"yyyyMM\").alias(\"CompetenciaMov\"),\n",
    "        F.col(\"região\").alias(\"Regiao\"),\n",
    "        F.col(\"uf\").alias(\"Uf\"),\n",
    "        F.col(\"município\").alias(\"Municipio\"),\n",
    "        F.col(\"seção\").alias(\"Secao\"),\n",
    "        F.col(\"subclasse\").alias(\"Subclasse\"),\n",
    "        F.col(\"saldomovimentação\").cast(\"integer\").alias(\"SaldoMovimentacao\"),\n",
    "        F.col(\"graudeinstrução\").cast(IntegerType()).alias(\"GrauInstrucao\"),\n",
    "        F.col(\"idade\").cast(IntegerType()).alias(\"Idade\"),\n",
    "        F.col(\"raçacor\").alias(\"RacaCor\"),\n",
    "        F.col(\"sexo\").alias(\"Sexo\"),\n",
    "        F.col(\"tamestabjan\").cast(\"integer\").alias(\"TamEstabJan\"),\n",
    "        F.col(\"file_path\").alias(\"FilePath\"),\n",
    "        F.col(\"source_modified_at\").alias(\"SourceModifiedAt\"),\n",
    "        F.col(\"is_active\").alias(\"IsActive\")\n",
    "    )\n",
    "\n",
    "    # Merge to update or insert into gold table\n",
    "    df_transformed.createOrReplaceTempView(\"source_data\")\n",
    "    spark.sql(f\"\"\"\n",
    "    MERGE INTO {gold_table} AS target\n",
    "    USING source_data AS source\n",
    "    ON target.FilePath = source.FilePath AND target.CompetenciaMov = source.CompetenciaMov\n",
    "    WHEN MATCHED AND target.SourceModifiedAt < source.SourceModifiedAt THEN\n",
    "        UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "    # Optimize the gold_table to reduce small files\n",
    "    spark.sql(f\"OPTIMIZE {gold_table}\")\n",
    "\n",
    "    # Record metadados\n",
    "    processed_files = df_gold_candidates.select(\"file_path\", \"source_modified_at\").distinct().collect()\n",
    "    df_processed = spark.createDataFrame(\n",
    "        [{\"file_path\": row[\"file_path\"], \"source_modified_at\": row[\"source_modified_at\"], \"processed_at\": datetime.now()} for row in processed_files],\n",
    "        schema=gold_meta_schema\n",
    "    )\n",
    "    df_processed.write.format(\"delta\").mode(\"append\").saveAsTable(gold_meta_table)\n",
    "\n",
    "    print(\"Gold zone updated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "c8ff1e00-35ab-4743-9399-2504f666c867",
    "default_lakehouse_name": "LK_SparkBlobMedallion",
    "default_lakehouse_workspace_id": "22faa9df-ae9d-4b3e-8f77-9d47b85702d9",
    "known_lakehouses": [
     {
      "id": "c8ff1e00-35ab-4743-9399-2504f666c867"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
