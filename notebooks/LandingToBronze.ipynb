{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3f5892-5bcd-4557-ab70-24fae0fce6e5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Incremental files ingestion from Azure Blob Storage to Microsoft Fabric Lakehouse using PySpark Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeeb130-7f71-4830-ac3b-e3b76a2ed097",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "> <br> This project uses the database provided by the government of Brazil called **Cadastro Geral de Empregados e Desempregados (CAGED)** (General Register of Employed and Unemployed).<br><br>\n",
    "> This dataset consists of monthly .txt files and is made available on ftp for public download.  <br>  \n",
    "> Download available at: <br> \n",
    "> <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df0eb5-39b0-4f9a-9daf-6ecf885da00b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**Notebook**: LandingToBronze  \n",
    "**Description**: This PySpark notebook performs incremental ingestion of files from Landing layer to the Bronze Layer (Delta) of the Lakehouse. It uses the ingestion date of each candidate file comparing to the historical already content to Delta table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42afac61-5d08-4c3c-bcf7-57872ab7b0b0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Set case sensitive for table and column names\n",
    "spark.conf.set('spark.sql.caseSensitive', True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f48cd91-67ab-436c-9a00-95f72096a55c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6294162-5f7e-4676-99ee-b110cfcba736",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Lakehouse paths\n",
    "landing_root_path         = f\"Files/Landing/CAGED\"\n",
    "landing_meta_table        = \"CagedLandingMeta\" # Track what was copied from Blob to Landing\n",
    "bronze_meta_table         = \"CagedBronzeMeta\"  # Control table to track from Landing (Files) to Bronze (Delta)\n",
    "bronze_table              = \"CagedBronze\"      # Bronze layer of data (as-is)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d850dc-3cce-453a-bb92-608f299e3260",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Prepare, create and load schemas and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f89f5-b979-409f-b34b-fe7d94a38984",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "bronze_meta_schema = StructType([\n",
    "    StructField(\"target_path\", StringType(), False),\n",
    "    StructField(\"source_modified_at\", TimestampType(), False),\n",
    "    StructField(\"source_size_mb\", FloatType(), False),\n",
    "    StructField(\"processed_at\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "# Create the bronze_meta_table (if not exists)\n",
    "spark.createDataFrame([], bronze_meta_schema) \\\n",
    "    .write.format(\"delta\") \\\n",
    "    .mode(\"ignore\") \\\n",
    "    .saveAsTable(bronze_meta_table)\n",
    "\n",
    "# Schema for bronze_table\n",
    "bronze_schema = StructType([\n",
    "    # CSV columns\n",
    "    StructField(\"competênciamov\", StringType(), True),  \n",
    "    StructField(\"região\", StringType(), True),         \n",
    "    StructField(\"uf\", StringType(), True),             \n",
    "    StructField(\"município\", StringType(), True),      \n",
    "    StructField(\"seção\", StringType(), True),          \n",
    "    StructField(\"subclasse\", StringType(), True),      \n",
    "    StructField(\"saldomovimentação\", StringType(), True),  \n",
    "    StructField(\"categoria\", StringType(), True),      \n",
    "    StructField(\"cbo2002ocupação\", StringType(), True), \n",
    "    StructField(\"graudeinstrução\", StringType(), True), \n",
    "    StructField(\"idade\", StringType(), True),          \n",
    "    StructField(\"horascontratuais\", StringType(), True), \n",
    "    StructField(\"raçacor\", StringType(), True),           \n",
    "    StructField(\"sexo\", StringType(), True),           \n",
    "    StructField(\"tipoempregador\", StringType(), True), \n",
    "    StructField(\"tipoestabelecimento\", StringType(), True), \n",
    "    StructField(\"tipomovimentação\", StringType(), True), \n",
    "    StructField(\"tipodedeficiência\", StringType(), True), \n",
    "    StructField(\"indtrabintermitente\", StringType(), True), \n",
    "    StructField(\"salário\", StringType(), True), \n",
    "    StructField(\"tamestabjan\", StringType(), True), \n",
    "    StructField(\"indicadoraprendiz\", StringType(), True), \n",
    "    StructField(\"origemdainformação\", StringType(), True), \n",
    "    StructField(\"competênciadec\", StringType(), True),  \n",
    "    StructField(\"indicadordeforadoprazo\", StringType(), True), \n",
    "    StructField(\"unidadesaláriocódigo\", StringType(), True), \n",
    "    StructField(\"valorsaláriofixo\", StringType(), True), \n",
    "    # Control columns\n",
    "    StructField(\"file_path\", StringType(), False),      # Landing path\n",
    "    StructField(\"source_modified_at\", TimestampType(), False), # Modification date\n",
    "    StructField(\"is_active\", BooleanType(), False)      # Active flag\n",
    "])\n",
    "\n",
    "# Create the bronze_table with schema\n",
    "spark.createDataFrame([], bronze_schema) \\\n",
    "    .write.format(\"delta\") \\\n",
    "    .mode(\"ignore\") \\\n",
    "    .saveAsTable(bronze_table)\n",
    "\n",
    "\n",
    "# Load landing_meta and bronze_meta\n",
    "df_landing_meta = spark.table(landing_meta_table)\n",
    "df_bronze_meta = spark.table(bronze_meta_table)\n",
    "\n",
    "# Consolidate the processed files\n",
    "df_bronze_meta_latest = (\n",
    "    df_bronze_meta\n",
    "    .groupBy(\"target_path\")\n",
    "    .agg(F.max(\"source_modified_at\").alias(\"last_processed_source_mtime\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586463ac-229a-46d1-b5d6-bf9bd2e6a5f0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Find candidates to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03cbe03-b006-4c30-8c10-53091e5e7586",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Identify candidate files to ingestion\n",
    "df_bronze_candidates = (\n",
    "    df_landing_meta\n",
    "    .join(\n",
    "        df_bronze_meta_latest, \n",
    "        df_landing_meta.target_path == df_bronze_meta_latest.target_path, \n",
    "        \"left\"\n",
    "    )\n",
    "    .filter(\n",
    "        F.col(\"last_processed_source_mtime\").isNull() |\n",
    "        (F.col(\"source_modified_at\") > F.coalesce(F.col(\"last_processed_source_mtime\"), F.lit(\"1970-01-01\")))\n",
    "    )\n",
    "    .select(\n",
    "        df_landing_meta.target_path,\n",
    "        df_landing_meta.source_modified_at,\n",
    "        df_landing_meta.source_size_mb\n",
    "    )\n",
    ")\n",
    "\n",
    "# Persist because we will use it multiple times\n",
    "df_bronze_candidates = df_bronze_candidates.persist()\n",
    "\n",
    "# Count candidates\n",
    "num_candidates = df_bronze_candidates.count()\n",
    "print(f\"Files to process from Landing to Bronze: {num_candidates}\")\n",
    "display(df_bronze_candidates.orderBy(\"target_path\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac4744-add9-4099-89b9-04480f4a5e24",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Process the load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5c22b-265e-4412-bef2-96d146f2c6c6",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Proper emptiness check in Spark\n",
    "if df_bronze_candidates.rdd.isEmpty():\n",
    "    print(\"Nothing to load. Skipping...\")\n",
    "else:\n",
    "    # 1) Collect only the PATHS (small footprint) – the heavy work is not here\n",
    "    candidate_paths = [r[\"target_path\"] for r in df_bronze_candidates.select(\"target_path\").toLocalIterator()]\n",
    "\n",
    "    # 2) Read ALL candidates at once (single Spark job), with minimal parsing.\n",
    "    #    If you have a known CSV schema for the data columns, set .schema(<schema_only_for_csv_cols>).\n",
    "    df_all = (\n",
    "        spark.read\n",
    "            .format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"sep\", \";\")\n",
    "            .option(\"encoding\", \"UTF-8\")\n",
    "            .load(candidate_paths)\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Normalize input_file_name() to match landing_meta paths:\n",
    "    # - Remove query strings (?version=..., ?flength=...)\n",
    "    # - Keep relative path from Files/...\n",
    "    # -------------------------------------------------------\n",
    "    df_all = (\n",
    "        df_all\n",
    "        .withColumn(\"file_path_raw\", F.input_file_name())\n",
    "        .withColumn(\"file_path_noqs\",  F.regexp_replace(F.col(\"file_path_raw\"), r\"\\?.*$\", \"\"))\n",
    "        .withColumn(\"file_path_rel\",   F.regexp_extract(F.col(\"file_path_noqs\"), r\"(?:^|/)(Files/.*)\", 1))\n",
    "    )\n",
    "\n",
    "    df_cand_norm = (\n",
    "        df_bronze_candidates\n",
    "        .withColumn(\"target_path_noqs\", F.regexp_replace(F.col(\"target_path\"), r\"\\?.*$\", \"\"))\n",
    "        .withColumn(\"target_path_rel\",  F.regexp_extract(F.col(\"target_path_noqs\"), r\"(?:^|/)(Files/.*)\", 1))\n",
    "        .select(\n",
    "            F.col(\"target_path_rel\"),\n",
    "            \"source_modified_at\",\n",
    "            \"source_size_mb\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Join to attach control metadata columns from landing_meta\n",
    "    df_all = (\n",
    "        df_all\n",
    "        .join(\n",
    "            df_cand_norm,\n",
    "            df_all.file_path_rel == df_cand_norm.target_path_rel,\n",
    "            \"left\"\n",
    "        )\n",
    "        .drop(\"target_path_rel\")\n",
    "        .withColumn(\"is_active\", F.lit(True))\n",
    "    )\n",
    "\n",
    "    # Persist bronze without start slash\n",
    "    df_all = df_all.withColumn(\"file_path\", F.col(\"file_path_rel\"))\n",
    "\n",
    "    # Final selection in order of the schema\n",
    "    df_all = df_all.select(\n",
    "        \"competênciamov\",\"região\",\"uf\",\"município\",\"seção\",\"subclasse\",\"saldomovimentação\",\"categoria\",\n",
    "        \"cbo2002ocupação\",\"graudeinstrução\",\"idade\",\"horascontratuais\",\"raçacor\",\"sexo\",\"tipoempregador\",\n",
    "        \"tipoestabelecimento\",\"tipomovimentação\",\"tipodedeficiência\",\"indtrabintermitente\",\"salário\",\n",
    "        \"tamestabjan\",\"indicadoraprendiz\",\"origemdainformação\",\"competênciadec\",\"indicadordeforadoprazo\",\n",
    "        \"unidadesaláriocódigo\",\"valorsaláriofixo\",\n",
    "        \"file_path\",\"source_modified_at\",\"is_active\"\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Single MERGE (batch) instead of per-file\n",
    "    # ------------------------------------------\n",
    "    df_all.createOrReplaceTempView(\"temp_files_batch\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {bronze_table} AS target\n",
    "        USING temp_files_batch AS source\n",
    "        ON target.file_path = source.file_path\n",
    "        WHEN MATCHED AND target.source_modified_at < source.source_modified_at\n",
    "            THEN UPDATE SET is_active = FALSE\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Write bronze_meta in batch\n",
    "    # ------------------------------------------\n",
    "    df_to_meta = (\n",
    "        df_bronze_candidates\n",
    "        .select(\n",
    "            F.col(\"target_path\").alias(\"target_path\"),\n",
    "            \"source_modified_at\",\n",
    "            \"source_size_mb\"\n",
    "        )\n",
    "        .withColumn(\"processed_at\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "    df_to_meta.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_meta_table)\n",
    "\n",
    "    print(f\"Processed files to Bronze: {num_candidates}\")\n",
    "\n",
    "    # Cleanup\n",
    "    df_bronze_candidates.unpersist()\n"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "c8ff1e00-35ab-4743-9399-2504f666c867",
    "default_lakehouse_name": "LK_SparkBlobMedallion",
    "default_lakehouse_workspace_id": "22faa9df-ae9d-4b3e-8f77-9d47b85702d9",
    "known_lakehouses": [
     {
      "id": "c8ff1e00-35ab-4743-9399-2504f666c867"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": null,
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "language": null,
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {
    "a42811df-a38c-4394-91a2-51ef17b0b477": {
     "persist_state": {
      "view": {
       "chartOptions": {
        "aggregationType": "sum",
        "binsNumber": 10,
        "categoryFieldKeys": [],
        "chartType": "bar",
        "isStacked": false,
        "seriesFieldKeys": [],
        "wordFrequency": "-1"
       },
       "tableOptions": {},
       "type": "details",
       "viewOptionsGroup": [
        {
         "tabItems": [
          {
           "key": "0",
           "name": "Table",
           "options": {},
           "type": "table"
          }
         ]
        }
       ]
      }
     },
     "sync_state": {
      "isSummary": false,
      "language": "scala",
      "table": {
       "rows": [
        {
         "0": "Files/Landing/CAGED/2020/CAGEDMOV202001.txt",
         "1": "2025-09-09 00:16:32",
         "2": "263.22"
        },
        {
         "0": "Files/Landing/CAGED/2020/CAGEDMOV202001.txt",
         "1": "2025-09-10 04:20:32",
         "2": "263.22"
        },
        {
         "0": "Files/Landing/CAGED/2020/CAGEDMOV202002.txt",
         "1": "2025-09-09 00:16:44",
         "2": "272.49"
        },
        {
         "0": "Files/Landing/CAGED/2020/CAGEDMOV202003.txt",
         "1": "2025-09-09 00:16:45",
         "2": "288.52"
        },
        {
         "0": "Files/Landing/CAGED/2020/CAGEDMOV202004.txt",
         "1": "2025-09-10 04:15:08",
         "2": "211.34"
        }
       ],
       "schema": [
        {
         "key": "0",
         "name": "target_path",
         "type": "string"
        },
        {
         "key": "1",
         "name": "source_modified_at",
         "type": "timestamp"
        },
        {
         "key": "2",
         "name": "source_size_mb",
         "type": "float"
        }
       ],
       "truncated": false
      },
      "wranglerEntryContext": {
       "dataframeType": "pyspark"
      }
     },
     "type": "Synapse.DataFrame"
    }
   },
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
