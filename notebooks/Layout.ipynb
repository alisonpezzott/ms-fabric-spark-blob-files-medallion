{"cells":[{"cell_type":"markdown","source":["# Incremental files ingestion from Azure Blob Storage to Microsoft Fabric Lakehouse using PySpark Notebooks"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"01481c52-6284-4ad0-88ae-7f613286c7c9"},{"cell_type":"markdown","source":["> <br> This project uses the database provided by the government of Brazil called **Cadastro Geral de Empregados e Desempregados (CAGED)** (General Register of Employed and Unemployed).<br><br>\n","> This dataset consists of monthly .txt files and is made available on ftp for public download.  <br>  \n","> Download available at: <br> \n","> <br> \n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"863ceb34-0a02-47b3-92b6-6662f963ba7c"},{"cell_type":"markdown","source":["**Notebook**: Layout.ipynb  \n","**Description**: This PySpark notebook performs ingestion of files of Layout from Azure Blob Storage to the Landing layer of the Lakehouse.  "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2c63b7d9-42d6-4036-a59d-48ca187db00c"},{"cell_type":"code","source":["# Imports\n","import re\n","\n","from datetime import datetime\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import *\n","\n","# Set case sensitive for table and column names\n","spark.conf.set('spark.sql.caseSensitive', True)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c6d713d4-53d8-40db-ba9e-2f56fb95d5d0"},{"cell_type":"markdown","source":["## Parameters  "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4039e5c4-14f6-4340-84c9-ec30f49c69b9"},{"cell_type":"code","source":["# Blob Storage\n","blob_storage_account_name = \"pezzott\"\n","blob_container_name       = \"caged-layout\"\n","blob_root_wasbs           = f\"wasbs://{blob_container_name}@{blob_storage_account_name}.blob.core.windows.net\" \n","\n","# Key vault secrets\n","key_vault_name            = \"pezzott\"\n","key_vault_secret_name     = \"blob-pezzott-caged-layout\"\n","\n","# Lakehouse paths\n","landing_root_path         = f\"Files/Landing/CAGED-Layout\"\n","landing_meta_table        = f\"CagedLayoutMeta\"\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fe2607f8-8948-461c-a613-8991bcaa5faf"},{"cell_type":"markdown","source":["## Shared access signature (SAS) for the WASBS driver"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f32ff010-0055-4f21-b546-cef93909b2bf"},{"cell_type":"code","source":["# SAS Token from Blob\n","sas_token = notebookutils.credentials.getSecret(\n","        f\"https://{key_vault_name}.vault.azure.net/\", \n","        key_vault_secret_name\n","    )\n","\n","# Configuring o SAS for the WASBS driver\n","spark.conf.set(\n","    f'fs.azure.sas.{blob_container_name}.{blob_storage_account_name}.blob.core.windows.net',\n","    sas_token\n",")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10fac15d-0275-446b-af3d-a6ff6fee9337"},{"cell_type":"markdown","source":["## List files from Blob"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"83c51403-1739-4643-9f12-02f9a19fe759"},{"cell_type":"code","source":["df_source_files = (\n","    spark.read.format(\"binaryFile\")\n","         .option(\"recursiveFileLookup\", \"true\")\n","         .load(f\"{blob_root_wasbs}\")\n","         .select(\n","             F.col(\"path\").alias(\"source_path\"),\n","             F.col(\"modificationTime\").alias(\"source_modified_at\"),  \n","             F.round(F.col(\"length\") / 1048576, 2).alias(\"source_size_mb\")                    \n","         )\n","         .orderBy('path')\n",")\n","\n","display(df_source_files.limit(10)) "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9b9c250f-5454-4d5c-b071-d9631b365aff"},{"cell_type":"markdown","source":["## Mount the target keeping same tree after container"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"87a19c85-4348-4935-a443-59214891093a"},{"cell_type":"code","source":["source_prefix = blob_root_wasbs.rstrip(\"/\") + \"/\"\n","remove_pattern = \"^\" + re.escape(source_prefix)\n","\n","file_name_col = F.regexp_extract(\"source_path\", r\"([^/]+)$\", 1)\n","df_source_files = (df_source_files\n","    .withColumn(\"relative_path\", F.regexp_replace(\"source_path\", remove_pattern, \"\"))\n","    .withColumn(\"target_path\",   F.concat(F.lit(landing_root_path + \"/\"), F.col(\"relative_path\")))\n",")\n","\n","df_source_files = df_source_files.select(\n","    \"source_path\",\n","    \"source_modified_at\",  \n","    \"source_size_mb\",         \n","    \"target_path\"\n",")\n","\n","display(df_source_files)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"60b91c66-9d49-45f1-98dc-b7602b427ee7"},{"cell_type":"markdown","source":["## Find the candidate files to copy"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"514e9c96-7438-4b9b-a24b-388454d83415"},{"cell_type":"code","source":["# Define schema to landing_meta_table\n","landing_meta_schema = StructType([\n","  StructField(\"source_path\",        StringType(),    False),\n","  StructField(\"source_modified_at\", TimestampType(), False),\n","  StructField(\"source_size_mb\",     FloatType(),     False),\n","  StructField(\"target_path\",        StringType(),    True ),\n","  StructField(\"copied_at\",          TimestampType(), True ),\n","])\n","\n","# Create the landing_meta_table (if not exists)\n","spark.createDataFrame([], landing_meta_schema) \\\n","    .write.format(\"delta\") \\\n","    .mode(\"ignore\") \\\n","    .saveAsTable(landing_meta_table)\n","\n","# Load as a named table\n","df_landing_meta = spark.table(landing_meta_table)\n","\n","# Consolidate latest copied info per source_path\n","df_landing_meta_latest = df_landing_meta.groupBy(\"source_path\").agg(\n","    F.max(\"source_modified_at\").alias(\"last_copied_source_mtime\")\n",")\n","\n","# Compute candidates to copy\n","df_candidates = (\n","    df_source_files\n","    .join(df_landing_meta_latest, on=\"source_path\", how=\"left\")\n","    .filter(\n","        F.col(\"source_modified_at\") > F.coalesce(F.col(\"last_copied_source_mtime\"), F.lit(\"1970-01-01\"))\n","    )\n","    .select(\n","        \"source_path\", \n","        \"source_modified_at\", \n","        \"source_size_mb\", \n","        \"target_path\"\n","    )\n",")    \n","\n","print(f\"Files to copy now (new or updated): {df_candidates.count()}\")\n","display(df_candidates.orderBy(\"source_path\"))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a66ad5d1-7276-4638-bce7-e5f7c5ffd75e"},{"cell_type":"markdown","source":["## Copy files from Blob to Landing Zone"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"38d882ae-ac5a-4ab6-b14d-0130df9a2891"},{"cell_type":"code","source":["if df_candidates.isEmpty():\n","    print(\"Nothing to copy. Skipping...\")\n","else:\n","    # Candidates from DataFrame to List\n","    candidates = df_candidates.select(\n","        \"source_path\", \n","        \"target_path\", \n","        \"source_modified_at\", \n","        \"source_size_mb\"\n","    ).collect()\n","\n","    # Copy files and prepare for update landing_meta_table\n","    copied_files = []\n","    for row in candidates:\n","        source_path = row[\"source_path\"]\n","        target_path = row[\"target_path\"]\n","        source_modified_at = row[\"source_modified_at\"]\n","        source_size = row[\"source_size_mb\"]\n","        \n","        try:\n","            # Copy file\n","            notebookutils.fs.fastcp(source_path, target_path)\n","            \n","            # Record metadata\n","            copied_files.append({\n","                \"source_path\": source_path,\n","                \"source_modified_at\": source_modified_at,\n","                \"source_size_mb\": source_size,\n","                \"target_path\": target_path,\n","                \"copied_at\": datetime.now() \n","            })\n","        except Exception as e:\n","            print(f\"Error copying {source_path} to {target_path}: {str(e)}\")\n","\n","    # Create DataFrame with copied files\n","    df_copied = spark.createDataFrame(\n","        copied_files, \n","        schema=spark.table(landing_meta_table).schema\n","    )\n","\n","    # write the metadata\n","    df_copied.write.format(\"delta\").mode(\"append\").saveAsTable(landing_meta_table)\n","\n","    # Show summary\n","    print(f\"Copied files: {len(copied_files)}\")\n","    display(df_copied.orderBy(\"source_path\"))\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f18822b7-f4ee-4a09-bbdf-7412d24a37f4"},{"cell_type":"markdown","source":["## Load Layout Files to Delta table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3dbf4da3-27c5-445f-be43-e1ea736daa2c"},{"cell_type":"code","source":["# Function to ingest and load each Layout file to table\n","def load_layout_table(\n","    table_name: str,\n","    codigo_is_integer: bool = True,\n","):\n","    schema = StructType([\n","        StructField(\"Codigo\",    IntegerType() if codigo_is_integer else StringType(), True),  \n","        StructField(\"Descricao\", StringType(),  True)\n","    ])\n","\n","    spark.createDataFrame([], schema) \\\n","        .write.format(\"delta\") \\\n","        .mode(\"ignore\") \\\n","        .saveAsTable(table_name)\n","    \n","    file_path = f\"{landing_root_path}/{table_name}.txt\"\n","\n","    df = spark.read \\\n","        .format(\"csv\") \\\n","        .schema(schema) \\\n","        .option(\"header\", \"true\") \\\n","        .option(\"sep\", \";\") \\\n","        .option(\"encoding\", \"UTF-8\") \\\n","        .load(file_path)\n","\n","    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n","    print(f\"Table {table_name} loaded successfully.\")  \n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb51f8f8-01cd-4df3-a914-f3244f7ecb64"},{"cell_type":"code","source":["load_layout_table(\"FaixaEtaria\") \n","load_layout_table(\"GrauInstrucao\")\n","load_layout_table(\"RacaCor\", False) \n","load_layout_table(\"Regiao\", False)\n","load_layout_table(\"Secao\", False)\n","load_layout_table(\"Sexo\", False)\n","load_layout_table(\"Subclasse\", False)\n","load_layout_table(\"TamEstabJan\")\n","load_layout_table(\"Uf\", False) \n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1b828b31-098f-451b-833d-4eab6f535069"},{"cell_type":"markdown","source":["### "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"084225f1-58ba-463d-85a8-67cdda8f4202"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"c8ff1e00-35ab-4743-9399-2504f666c867"}],"default_lakehouse":"c8ff1e00-35ab-4743-9399-2504f666c867","default_lakehouse_name":"LK_SparkBlobMedallion","default_lakehouse_workspace_id":"22faa9df-ae9d-4b3e-8f77-9d47b85702d9"}}},"nbformat":4,"nbformat_minor":5}